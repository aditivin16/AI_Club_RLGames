{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/07Ibsj3RJVVviljI6R43",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditivin16/AI_Club_RLGames/blob/main/aditi_a2c_2_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoZc3C9mW-wz",
        "outputId": "e65cb549-34af-4147-ce98-4e652b78c74d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.1.1)\n",
            "Episode 1/100, Net Reward: -411.49345164864155\n",
            "Episode 2/100, Net Reward: -311.7580186458903\n",
            "Episode 3/100, Net Reward: -373.7924111866449\n",
            "Episode 4/100, Net Reward: -906.9574236441428\n",
            "Episode 5/100, Net Reward: -501.1808343031242\n",
            "Episode 6/100, Net Reward: -666.1993688693389\n",
            "Episode 7/100, Net Reward: -337.40778379225765\n",
            "Episode 8/100, Net Reward: -793.9006421259501\n",
            "Episode 9/100, Net Reward: -709.7242550717442\n",
            "Episode 10/100, Net Reward: -503.5136600800073\n",
            "Episode 11/100, Net Reward: -706.213082570204\n",
            "Episode 12/100, Net Reward: -671.1862284566914\n",
            "Episode 13/100, Net Reward: -585.3914560761892\n",
            "Episode 14/100, Net Reward: -666.7249006302175\n",
            "Episode 15/100, Net Reward: -799.1695983854349\n",
            "Episode 16/100, Net Reward: -568.7896248154786\n",
            "Episode 17/100, Net Reward: -724.3310081898865\n",
            "Episode 18/100, Net Reward: -813.9319642144839\n",
            "Episode 19/100, Net Reward: -446.97495001615204\n",
            "Episode 20/100, Net Reward: -388.50514772954864\n",
            "Episode 21/100, Net Reward: -442.33105176852723\n",
            "Episode 22/100, Net Reward: -541.8360603512867\n",
            "Episode 23/100, Net Reward: -393.8693275459231\n",
            "Episode 24/100, Net Reward: -403.6125995450671\n",
            "Episode 25/100, Net Reward: -823.1498565304025\n",
            "Episode 26/100, Net Reward: -402.68434431855746\n",
            "Episode 27/100, Net Reward: -441.08102514135027\n",
            "Episode 28/100, Net Reward: -366.1918508485442\n",
            "Episode 29/100, Net Reward: -571.0875306223841\n",
            "Episode 30/100, Net Reward: -386.80590751502393\n",
            "Episode 31/100, Net Reward: -491.4833303377614\n",
            "Episode 32/100, Net Reward: -787.2414660199086\n",
            "Episode 33/100, Net Reward: -740.0119956273219\n",
            "Episode 34/100, Net Reward: -486.63029126258243\n",
            "Episode 35/100, Net Reward: -804.1804998562656\n",
            "Episode 36/100, Net Reward: -436.98792544439897\n",
            "Episode 37/100, Net Reward: -926.1545956123848\n",
            "Episode 38/100, Net Reward: -857.3828993897544\n",
            "Episode 39/100, Net Reward: -582.4656314447709\n",
            "Episode 40/100, Net Reward: -668.4607158979966\n",
            "Episode 41/100, Net Reward: -695.502857363363\n",
            "Episode 42/100, Net Reward: -972.7259405900467\n",
            "Episode 43/100, Net Reward: -667.6563162316205\n",
            "Episode 44/100, Net Reward: -463.2809021155445\n",
            "Episode 45/100, Net Reward: -336.6791320045746\n",
            "Episode 46/100, Net Reward: -571.4937535714862\n",
            "Episode 47/100, Net Reward: -411.1594551742023\n",
            "Episode 48/100, Net Reward: -321.79891662281335\n",
            "Episode 49/100, Net Reward: -566.1684999984436\n",
            "Episode 50/100, Net Reward: -346.60064070871204\n",
            "Episode 51/100, Net Reward: -667.617353375615\n",
            "Episode 52/100, Net Reward: -788.7389023909096\n",
            "Episode 53/100, Net Reward: -421.475214111126\n",
            "Episode 54/100, Net Reward: -461.7245681252033\n",
            "Episode 55/100, Net Reward: -795.4792893499492\n",
            "Episode 56/100, Net Reward: -446.0862577266442\n",
            "Episode 57/100, Net Reward: -551.8059502987185\n",
            "Episode 58/100, Net Reward: -616.5452696854512\n",
            "Episode 59/100, Net Reward: -482.38977047379683\n",
            "Episode 60/100, Net Reward: -621.0139648148072\n",
            "Episode 61/100, Net Reward: -781.795374628226\n",
            "Episode 62/100, Net Reward: -816.7836716129316\n",
            "Episode 63/100, Net Reward: -459.758481191614\n",
            "Episode 64/100, Net Reward: -465.89989726648855\n",
            "Episode 65/100, Net Reward: -737.3210805482785\n",
            "Episode 66/100, Net Reward: -553.5869187575296\n",
            "Episode 67/100, Net Reward: -807.9423083820909\n",
            "Episode 68/100, Net Reward: -698.799018934894\n",
            "Episode 69/100, Net Reward: -928.4214927273626\n",
            "Episode 70/100, Net Reward: -334.02423937098524\n",
            "Episode 71/100, Net Reward: -569.574718625774\n",
            "Episode 72/100, Net Reward: -497.5515992567658\n",
            "Episode 73/100, Net Reward: -664.8830791564129\n",
            "Episode 74/100, Net Reward: -794.2481333887861\n",
            "Episode 75/100, Net Reward: -737.5033425120995\n",
            "Episode 76/100, Net Reward: -504.9697250668362\n",
            "Episode 77/100, Net Reward: -631.4362952891414\n",
            "Episode 78/100, Net Reward: -908.0866524501726\n",
            "Episode 79/100, Net Reward: -753.0798925300571\n",
            "Episode 80/100, Net Reward: -539.0960120350705\n",
            "Episode 81/100, Net Reward: -702.6442591903002\n",
            "Episode 82/100, Net Reward: -435.9655473234194\n",
            "Episode 83/100, Net Reward: -463.83445554449804\n",
            "Episode 84/100, Net Reward: -637.7907370343031\n",
            "Episode 85/100, Net Reward: -469.4850873699076\n",
            "Episode 86/100, Net Reward: -1009.4127376435389\n",
            "Episode 87/100, Net Reward: -560.738727036201\n",
            "Episode 88/100, Net Reward: -499.27309146720773\n",
            "Episode 89/100, Net Reward: -530.734715422533\n",
            "Episode 90/100, Net Reward: -523.5430844596076\n",
            "Episode 91/100, Net Reward: -791.8500615353647\n",
            "Episode 92/100, Net Reward: -522.7740520469171\n",
            "Episode 93/100, Net Reward: -478.1831895337359\n",
            "Episode 94/100, Net Reward: -939.686727913598\n",
            "Episode 95/100, Net Reward: -487.9399603307967\n",
            "Episode 96/100, Net Reward: -462.54433931592473\n",
            "Episode 97/100, Net Reward: -575.6600566432483\n",
            "Episode 98/100, Net Reward: -324.3146910208628\n",
            "Episode 99/100, Net Reward: -356.2543551382454\n",
            "Episode 100/100, Net Reward: -743.1397674179495\n"
          ]
        }
      ],
      "source": [
        "!pip3 install swig\n",
        "!pip3 install gym[box2d]\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "actor_lr = 0.01\n",
        "critic_lr = 0.01\n",
        "gamma = 0.99\n",
        "num_episodes = 100\n",
        "\n",
        "\n",
        "# Define the actor and critic networks\n",
        "actor = nn.Sequential(nn.Linear(8, 64),nn.ReLU(),nn.Linear(64, 64),nn.ReLU(),nn.Linear(64, 4),nn.Softmax(dim=-1)) #softmax to convert to action probabilities\n",
        "critic = nn.Sequential(nn.Linear(8, 64),nn.ReLU(),nn.Linear(64, 64),nn.ReLU(),nn.Linear(64, 1))\n",
        "\n",
        "# Defining the optimizer for actor and critic\n",
        "optimizer_actor = optim.Adam(actor.parameters(), lr=actor_lr)\n",
        "optimizer_critic = optim.Adam(critic.parameters(), lr=critic_lr)\n",
        "\n",
        "# making the environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "rewards=[]\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    net_reward = 0\n",
        "    done=False\n",
        "    for i in range(300):\n",
        "        # Sample an action from the actor\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "        action_probs = actor(state_tensor)\n",
        "        action_distribution = torch.distributions.Categorical(action_probs)\n",
        "        action = action_distribution.sample()\n",
        "        next_state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "        # Calculating the TD error\n",
        "        state_value = critic(state_tensor)\n",
        "        next_state_value = critic(torch.tensor(next_state, dtype=torch.float32))\n",
        "        td_error = reward + gamma * next_state_value - state_value\n",
        "\n",
        "        # Updating the critic\n",
        "        loss_critic = td_error.pow(2)\n",
        "        optimizer_critic.zero_grad()\n",
        "        loss_critic.backward()\n",
        "        optimizer_critic.step()\n",
        "\n",
        "        # Updating the actor using the TD error as a baseline\n",
        "        log_prob = action_distribution.log_prob(action)\n",
        "        loss_actor = -log_prob * td_error.detach()\n",
        "        optimizer_actor.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        optimizer_actor.step()\n",
        "\n",
        "        net_reward += reward\n",
        "        state = next_state\n",
        "        if done== True:\n",
        "          break\n",
        "\n",
        "\n",
        "    print(f\"Episode {episode + 1}/{num_episodes}, Net Reward: {net_reward}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ]
    }
  ]
}

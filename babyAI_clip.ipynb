{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditivin16/AI_Club_RLGames/blob/main/aditi_clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7EaXu9f0L0_",
        "outputId": "786a5686-8d27-49a9-9c2c-cc6f5c709aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JTdEPiv0IiI"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB5M4AuFzhBt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.distributions as distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HdIBvRE0wMu",
        "outputId": "95067235-d4bb-440b-8bd5-f39625631af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: minigrid in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from minigrid) (1.23.5)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from minigrid) (0.29.1)\n",
            "Requirement already satisfied: pygame>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from minigrid) (2.5.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minigrid) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minigrid) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minigrid) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install minigrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSVdyTeV0ofN",
        "outputId": "fc32602e-03a5-4839-c103-6afdb67e66e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "import minigrid\n",
        "from minigrid.wrappers import ImgObsWrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYwfYDYf2HpG"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmCkWGdq1cEx"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"BabyAI-GoToRedBallGrey-v0\", render_mode='rgb_array')\n",
        "n_actions=7 #for this env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FpFo_rg2aNo",
        "outputId": "726a0ff1-3d90-4f47-98cd-1f0877d99bec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Creating the CLIP model\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "layer1 = torch.nn.Linear(1280, 100)\n",
        "layer2 = torch.nn.Linear(100, 128)\n",
        "relu = torch.nn.ReLU()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47MGnUokEy1j"
      },
      "outputs": [],
      "source": [
        "def clip_model(text, image, direction):\n",
        "    encoded = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
        "    clip_output = model(**encoded)\n",
        "    direction_tensor = torch.tensor([direction])\n",
        "    base_output = torch.cat((clip_output.text_model_output.pooler_output, clip_output.vision_model_output.pooler_output), dim=1)\n",
        "    l1 = layer1(base_output)\n",
        "    r = relu(l1)\n",
        "    l2 = layer2(r)\n",
        "    return l2\n",
        "#here clip_output.text_model_output} will have information about the text encodings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4Hm8XRyI-iL"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "actor_lr = 0.001\n",
        "critic_lr = 0.001\n",
        "gamma = 0.99\n",
        "num_episodes = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9IDGeYGJo8B"
      },
      "outputs": [],
      "source": [
        "# Define the actor and critic networks\n",
        "actor = nn.Sequential(\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 7),  # Adapted to the action space (7 actions in BabyAI)\n",
        ")\n",
        "\n",
        "\n",
        "critic = nn.Sequential(\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 1)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwLOaaXSJ-DW"
      },
      "outputs": [],
      "source": [
        "# Defining the optimizer for actor and critic\n",
        "optimizer_actor = optim.Adam(actor.parameters(), lr=actor_lr)\n",
        "optimizer_critic = optim.Adam(critic.parameters(), lr=critic_lr)\n",
        "rewards=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyjZG0yLUmBW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, clear_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cppljt7xGaow",
        "outputId": "9b01ae5f-52e0-4788-d174-d14a6de212e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "max_step=100\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    net_reward = 0\n",
        "    done = False\n",
        "    trunc=False\n",
        "    i=0\n",
        "    step=0\n",
        "\n",
        "    while not done and step<max_step and not trunc:\n",
        "        step=step+1\n",
        "        if(i==0):\n",
        "          state_image = state[0]['image']  # Access the image\n",
        "          state_image = torch.tensor(state_image, dtype=torch.float32)\n",
        "          text_encoding = state[0]['mission']\n",
        "          i=1\n",
        "        else:\n",
        "          state_image = state['image']  # Access the image\n",
        "          state_image = torch.tensor(state_image, dtype=torch.float32)\n",
        "          text_encoding = state['mission']\n",
        "\n",
        "\n",
        "\n",
        "        # We give a text prompt of mission\n",
        "        direction = torch.randint(0, 7, (1,))  # Randomly sample an action as a baseline\n",
        "        encoding = clip_model(text_encoding, state_image, direction)\n",
        "        action_probs = actor(encoding)\n",
        "        action_distribution = distributions.Categorical(logits=action_probs)\n",
        "        action = action_distribution.sample().item()\n",
        "        # next_state, reward, done, info = env.step(action)\n",
        "        print(action)\n",
        "        next_state,reward,done,trunc,_= env.step(action)\n",
        "        #print( reward, done, action, truc )\n",
        "        state = next_state  # Update the state for the next iteration\n",
        "        # Calculating the TD error\n",
        "        state_value2 = critic(encoding)\n",
        "        state_image2 = state['image']  # Access the image\n",
        "        state_image2 = torch.tensor(state_image, dtype=torch.float32)\n",
        "        text_encoding2 = state['mission']  # We give a text prompt of mission\n",
        "        direction2 = torch.randint(0, 7, (1,))  # Randomly sample an action as a baseline\n",
        "        encoding2 = clip_model(text_encoding2, state_image2, direction2)\n",
        "\n",
        "        next_state_value = critic(encoding2)\n",
        "        td_error = reward + gamma * (next_state_value - state_value2)\n",
        "\n",
        "        # Updating the critic\n",
        "        loss_critic = td_error.pow(2)\n",
        "        optimizer_critic.zero_grad()\n",
        "        # loss_critic.backward()\n",
        "        # optimizer_critic.step()\n",
        "\n",
        "        # Updating the actor using the TD error as a baseline\n",
        "        log_prob = action_distribution.log_prob(torch.tensor(action))\n",
        "        loss_actor = -log_prob * td_error.detach() + loss_critic\n",
        "        optimizer_actor.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        optimizer_actor.step()\n",
        "        optimizer_critic.step()\n",
        "        plt.imshow(env.render())\n",
        "        plt.show()\n",
        "        plt.pause(1e-3)\n",
        "        clear_output()\n",
        "\n",
        "\n",
        "        net_reward += reward\n",
        "\n",
        "    print(f\"Episode {episode + 1}/{num_episodes}, Net Reward: {net_reward}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmDvaj+vVQwTIxeEwQzs4g",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
